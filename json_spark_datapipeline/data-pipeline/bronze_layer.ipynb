{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "from pyspark.sql.types import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criacao de tabelas\n",
    "def create_table(table_name:str, has_input:bool, in_fld:str='/FileStore/tables/raw_file/delta'):\n",
    "    if has_input:\n",
    "        # Deleta a tabela caso exista\n",
    "        spark.sql(f\"\"\"\n",
    "        DROP TABLE IF EXISTS {table_name}\"\"\")\n",
    "        # cria uma nova tabela com os dados delta tratados anteriormente\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{in_fld}'\n",
    "        \"\"\")\n",
    "        # Por termos uma serie de arquivos pequenos, 'e necessario\n",
    "        # rodar o optimize para reduzir arquivos pequenos em maiores\n",
    "        spark.sql(\"\"\"\n",
    "        OPTIMIZE default.raw\n",
    "        \"\"\")\n",
    "    else:\n",
    "        # Deleta a tabela caso exista\n",
    "        spark.sql(f\"\"\"\n",
    "        DROP TABLE IF EXISTS {table_name};\n",
    "        \"\"\")\n",
    "        # cria uma nova tabela com os dados delta tratados anteriormente\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name}\n",
    "            (\n",
    "            dateTimeReference timestamp,\n",
    "            modifiedDate timestamp\n",
    "            idRetailerSKU string,\n",
    "            seller string,\n",
    "            retailerProductCode string,\n",
    "            retailerAverageRating float,\n",
    "            retailerRatingCount int,\n",
    "            retailerPrice float, \n",
    "            manufacturerPrice float, \n",
    "            priceVariation float,\n",
    "            available boolean,\n",
    "            unavailable boolean,\n",
    "            notListed boolean,\n",
    "            titleFlag boolean,\n",
    "            titlePercentage float\n",
    "            );\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # primeiramente, foi 'e aconselhado passar a estrutura de leitura do json\n",
    "    # quando for realizar a leitura do arquivo pois ele ja trata no pyspark\n",
    "    # possiveis problemas de tipagem ou inconsistencia de dados\n",
    "    schema = StructType([\n",
    "        StructField(\"assortment\", ArrayType(\n",
    "            StructType([\n",
    "                StructField('dateTimeReference', TimestampType(), True),\n",
    "                StructField('modifiedDate', TimestampType(), True),\n",
    "                StructField('idRetailerSKU', StringType(), True),\n",
    "                StructField('seller', StringType(), True),\n",
    "                StructField('retailerProductCode', StringType(), True),\n",
    "                StructField(\"retailerAverageRating\", FloatType(), True),\n",
    "                StructField(\"retailerRatingCount\", IntegerType(), True),\n",
    "                StructField('retailerPrice', FloatType(),True),\n",
    "                StructField('manufacturerPrice', FloatType(),True),\n",
    "                StructField('priceVariation', FloatType(),True),\n",
    "                StructField('available', BooleanType(),True),\n",
    "                StructField('unavailable', BooleanType(),True),\n",
    "                StructField('notListed', BooleanType(),True),\n",
    "                StructField('titleFlag', BooleanType(),True),\n",
    "                StructField('titlePercentage', FloatType(),True),\n",
    "            ])\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # chamamos a pasta de arquivos e inserimos todos os valores linha a linha em\n",
    "    # um rdd com os dados\n",
    "    df = spark.read.schema(schema).json( \"/FileStore/tables/raw_file/data-eng-test/\")\n",
    "    # aqui 'e feita a conversao do tipo de arquivo para delta\n",
    "    # se ja existem arquivos dentro da pasta delta\n",
    "    if dbutils.fs.ls(\"/FileStore/tables/raw_file/delta_lake/\"):\n",
    "        dbutils.fs.rm(\"/FileStore/tables/raw_file/delta_lake\",True)\n",
    "    df.write.format('delta').mode('overwrite').save('/FileStore/tables/raw_file/delta_lake')\n",
    "# tratamento da excecao de caso nao haja a pasta dos arquivos selecionados\n",
    "except Exception as e:\n",
    "    print(f'Houve um erro na extracao doas dados. Erro: {e}')\n",
    "# mensagem de finalizacao do notebook\n",
    "else: \n",
    "    # Cria tabela com os dados presentes dos arquivos delta gerados\n",
    "    # no notebook anterior\n",
    "    try:\n",
    "        create_table(table_name='default.bronze', \n",
    "                    has_input=True, \n",
    "                    in_fld='/FileStore/tables/raw_file/delta_lake')\n",
    "    except Exception as e:\n",
    "        print(\"Nao foi possivel fazer a carga da tabela\" + '\\n' + f'Erro : {e}')\n",
    "    else:\n",
    "        print('Dados carregados com sucesso!')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela com os dados presentes dos arquivos delta gerados\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
